{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains some functions from the [utils folder](utils/__init__.py) which you can use to clean, convert, transform and scale the data.\\\n",
    "Included will be the function documentation, how to import the function (relative to the project root), and a small example for any notable function.\\\n",
    "Make sure the [requirements](requirements.txt) are installed into your current python environment (see [here](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/) for more information)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_cleaning import load_and_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [function](utils/data_cleaning.py#L8) cleans the raw data by dropping unnecessary columns and rows and converting the target price column to a float.\n",
    "\n",
    "**Parameters:**\n",
    "- *drop_columns (bool):* Whether to drop any columns.\n",
    "- *drop_rows (bool):* Whether to drop any rows. It will however, always convert the target price column to a float for the sake of visualization.\n",
    "- *\\*\\*kwargs:* Additional keyword arguments to pass to the drop_features and handle_price functions.\n",
    "\n",
    "**Returns:**\n",
    "- *pd.DataFrame*: The cleaned data as a pandas DataFrame.\n",
    "\n",
    "---\n",
    "\n",
    "Aditional arguments which can be passed through the *\\*\\*kwargs* argument, must be key=value pairs (**e.g.:** 'quantile=0.8, verbose=True'; **not:** '0.8, True'):\n",
    "- *quantile: float = 0.99*, the quantile to use for outlier removal.\n",
    "- *verbose: bool = False*, whether to print some meaningfull information during computation.\n",
    "- *remove_redundant_feats: bool = True*, whether to remove redundant features, specified in *manual_redundant_feats* and *add_redundant_feats*.\n",
    "- *manual_redundant_feats: list = [...]*, a list of features to remove. A predefined list is the default value, for exactly which features are removed, see the [function](utils/data_cleaning.py#L73).\n",
    "- *add_redundant_feats: list = []*, optional list of features to add to the *manual_redundant_feats* list.\n",
    "- *remove_high_corr_feats: bool = True*, whether to remove features with a high correlation to other features.\n",
    "- *corr_threshold: float = 0.8*, the threshold for the correlation coefficient above which features are removed.\n",
    "- *remove_missing_feats: bool = True*, whether to remove features with more than *missing_threshold* missing values.\n",
    "- *missing_threshold: float = 0.5*, the threshold for the percentage of missing values above which features are removed.\n",
    "- *remove_single_value_feats: bool = True*, whether to remove features with only one unique value.\n",
    "- *remove_text_feats: bool = True*, whether to remove features with text values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the data as is (no cleaning or removal), useful for plotting the data before and after cleaning\n",
    "data = load_and_clean(False, False)\n",
    "\n",
    "# Load the data, with different thresholds and columns to remove\n",
    "data = load_and_clean(add_redundant_feats=['host_location', 'latitude'], corr_threshold=0.7, missing_threshold=.4)\n",
    "\n",
    "# Print which columns / how many rows are removed during cleaning\n",
    "data = load_and_clean(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pipeline import create_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below is used to remove warnings from the notebook output, you can ignore it\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [function](utils/pipeline.py#L18) creates a Sci-kit Learn pipeline for preprocessing the given dataframe. It uses information on the feature types specified [here](utils/_feature_types.py) to determine the preprocessing steps to be applied per feature.\n",
    "\n",
    "**Parameters:**\n",
    "- *df (pd.DataFrame):* The dataframe to preprocess.\n",
    "- *model (BaseEstimator)*: The model to be used in the pipeline. Default is DecisionTreeRegressor(random_state=24).\n",
    "- *filter (BaseEstimator):* The filter to be used in the pipeline. Default is SelectKBest(f_regression, k=10).\n",
    "- *convert (bool):* Whether to convert the features or not. Default is True.\n",
    "- *impute (bool):* Whether to impute the missing values or not. Default is True.\n",
    "- *encode (bool):* Whether to encode the categorical features or not. Default is True.\n",
    "- *scale (bool):* Whether to scale the features or not. Default is True.\n",
    "- *feature_selection (bool):* Whether to select the features or not. Default is True.\n",
    "- *model_selection (bool):* Whether to add an estimator model or not. If True, an estimator model should be provided in *model*. Default is True.\n",
    "    \n",
    "**Returns:**\n",
    "- *Pipeline:* The pipeline for preprocessing the dataframe.\n",
    "\n",
    "---\n",
    "\n",
    "The function can both be used to preprocess a dataframe in order to visualize the data after cleaning, or to preprocess the data and attach it to a model.\\\n",
    "When the *model_selection* parameter is set to True, the function will return a pipeline which can be used to fit data to a model (*.fit()* / *.predict()* / *.fit_predict()*).\\\n",
    "When the *model_selection* parameter is set to False, the function will return a pipeline which can be used to fit and transform the data (*.fit()* / *.transform()* / *.fit_transform()*).\n",
    "\n",
    "The pipeline is constructed from both Sci-kit Learn native transformers, as well as custom converters and encoders. For reference or additional declarations, you can find the custom converters [here](utils/_custom_converters.py) and the custom encoders [here](utils/_custom_encoders.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target variable\n",
    "# In practice, you would typically split the data into training and testing sets, but for illustration purposes, we'll use the entire dataset\n",
    "X = data.drop(columns=['price'])\n",
    "y = data['price']\n",
    "\n",
    "# Preprocess the data to only convert features into interpretable values\n",
    "# This will convert specific columns and return the transformed data as a pandas DataFrame\n",
    "pipe = create_pipeline(data, impute=False, encode=False, scale=False, feature_selection=False, model_selection=False)\n",
    "pipe.fit_transform(X)\n",
    "\n",
    "# Preprocess the data to convert, impute, and encode all features into numeric instances\n",
    "# This function requires fit_transform() to be called since it requires fitted data for target encoding\n",
    "# The returned pandas DataFrame will contain the fully transformed dataset\n",
    "pipe = create_pipeline(data, scale=False, feature_selection=False, model_selection=False)\n",
    "pipe.fit_transform(X, y)\n",
    "\n",
    "# Create the full pipeline to fit data to the model, with an interactive graphical interface\n",
    "# As the function incorporates a model, we can only call fit() on the pipeline\n",
    "pipe = create_pipeline(data) # Optional: specify the model to use as second parameter, import this model seperately from the sklearn library\n",
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted pipeline can be used to make predictions on new data and compute metrics (note this can only be done when the function argument *model_selection* is set to *True*).\n",
    "\n",
    "You can find a full list of the available metrics [here](https://scikit-learn.org/stable/modules/model_evaluation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Normally, you would use the test set for making predictions, but in this example we'll use the training set for simplicity\n",
    "# Due to training and predicting on the same set, we are overfitting and getting a near perfect score, which is not realistic in practice\n",
    "y_pred = pipe.predict(X)\n",
    "print(f\"\"\"\n",
    "      Values of Target 'price':\n",
    "      {'-' * 80}\n",
    "      predicted y values:\n",
    "      {y_pred.tolist()[:10]}\n",
    "\n",
    "      actual y values:\n",
    "      {y.tolist()[:10]}\n",
    "\n",
    "\n",
    "\n",
    "      Model Performance Metrics:\n",
    "      {'-' * 80}\n",
    "      {'mean squared error:': <30} {mean_squared_error(y, y_pred):.5f}\n",
    "      {'mean absolute error:': <30} {mean_absolute_error(y, y_pred):.5f}\n",
    "      {'r2 score:': <30} {r2_score(y, y_pred):.5f}\n",
    "      \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import save_figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [function](utils/__init__.py#L6) can be used to save a figure to the `figures` folder. It will automatically create the folder or subfolder if it does not exist in the project root.\n",
    "\n",
    "**Parameters:**\n",
    "- *fig (plt.Figure):* The figure object to save.\n",
    "- *name (str):* The name of the figure with or without the extension. The default extension is .png.\n",
    "- *subfolder (str):* The subfolder to save the figure to. If none is provided, the figure will be saved to the root folder.\n",
    "- *dpi (int):* The resolution of the figure. Defaults to 300.\n",
    "- *bbox_inches (str):* The bounding box of the figure. Defaults to tight.\n",
    "- *\\*\\*kwargs:* Additional keyword arguments to pass to the savefig function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the figure of choice\n",
    "# This example will create a random smiley plot (courtesy of https://gist.github.com/bbengfort/dd9d8027a37f3a96c44323a8303520f0)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(1,1,1, aspect=1)\n",
    "\n",
    "ax.scatter([.5],[.5], c='#FFCC00', s=120000, label=\"face\")\n",
    "ax.scatter([.35, .65], [.63, .63], c='k', s=1000, label=\"eyes\")\n",
    "\n",
    "X = np.linspace(.3, .7, 100)\n",
    "Y = 2* (X-.5)**2 + 0.30\n",
    "\n",
    "ax.plot(X, Y, c='k', linewidth=8, label=\"smile\")\n",
    "\n",
    "ax.set_xlim(0,1)\n",
    "ax.set_ylim(0,1)\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "# Save the figure using the custom function\n",
    "save_figure(fig, 'smiley', subfolder='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
